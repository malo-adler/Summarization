{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install and import all the dependencies\n",
        "\n",
        "!pip install contractions\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "import codecs\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from transformers import AutoTokenizer, GPT2Model\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "XkkoyNozcTca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Kshp2SaqB_ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To fill with the path where the data is stored\n",
        "\n",
        "os.chdir(\"drive/My Drive/Colab Notebooks/Language\")\n",
        "!ls"
      ],
      "metadata": {
        "id": "swOb4WKsDFuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing of the data\n",
        "\n",
        "def pre_process(input_filename, text_column_name, summary_column_name, output_filename):\n",
        "    \"\"\"\n",
        "    Pre-processes a .csv file containing texts and summaries, by expanding english contractions \n",
        "    and storing the obtained data in a new .csv file.\n",
        "\n",
        "    Parameters: \n",
        "        input_filename (str)     : The name of the .csv file that is to be pre-processed\n",
        "        text_column_name (str)   : The name of the column of the .csv file which contains the texts\n",
        "        summary_column_name (str): The name of the column of the .csv file which contains the summaries\n",
        "        output_filename (str)    : The name of the .csv file where the pre-processed data is to be stored\n",
        "    \"\"\"\n",
        "\n",
        "    with open(input_filename, encoding='utf-8') as f:\n",
        "        df_init = pd.read_csv(f)\n",
        "    f.close()\n",
        "    print(\"Init pandas file\")\n",
        "\n",
        "    # Remove all the useless columns, keep only the texts and the summaries\n",
        "    df = df_init[[summary_column_name, text_column_name]].copy()\n",
        "    df.rename(columns = {text_column_name: 'Initial_text'}, inplace = True) \n",
        "    df.rename(columns = {summary_column_name: 'Initial_summary'}, inplace = True)\n",
        "    df.dropna(inplace=True) # Remove rows with NaN values\n",
        "    print(\"Remove useless columns: Done\")\n",
        "\n",
        "    # Contractions expansion\n",
        "    df['Text_not_contracted'] = df['Initial_text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "    df['Text'] = [' '.join(l) for l in df['Text_not_contracted']]\n",
        "    df['Summary_not_contracted'] = df['Initial_summary'].apply(lambda x: [contractions.fix(word) for word in str(x).split()])\n",
        "    df['Summary'] = [' '.join(l) for l in df['Summary_not_contracted']]\n",
        "    print('Contractions expansion: Done')\n",
        "\n",
        "    # Save the new .csv file\n",
        "    df[['Text', 'Summary']].to_csv(output_filename)"
      ],
      "metadata": {
        "id": "D0Hli6eHBOlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data is originally stored in 'Reviews.csv', the processed version is stored in 'Reviews_preprocessed.csv'\n",
        "\n",
        "pre_process('Reviews.csv', 'Text', 'Summary', 'Reviews_preprocessed.csv')"
      ],
      "metadata": {
        "id": "wfCe7TPMBVrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GPT-2 tokenizer, and a pad token\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "sZYVwedAIBSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lJTc4xx6nR-"
      },
      "outputs": [],
      "source": [
        "# Definition of the custom dataset class \n",
        "\n",
        "class SummarizationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, max_text_length=900, max_summary_length=60, stop = 10000):\n",
        "     \n",
        "\n",
        "        self.max_text_length = max_text_length\n",
        "        self.max_summary_length = max_summary_length\n",
        "\n",
        "        print(\"Creating Mapping\")\n",
        "\n",
        "        with open(filename, encoding='utf-8') as f:\n",
        "            df = pd.read_csv(f).head(stop)\n",
        "        f.close()\n",
        "\n",
        "        self.total_lines = len(df) - 1\n",
        "\n",
        "        pad_token_id = tokenizer(tokenizer.pad_token)['input_ids'][0]\n",
        "        self.input_ids = torch.ones((self.total_lines, self.max_text_length),dtype = torch.long) * pad_token_id#self.input_ids = torch.ones((self.total_lines, self.max_summary_length + self.max_text_length),dtype = torch.long) * pad_token_id\n",
        "        self.label_ids = torch.zeros((self.total_lines, self.max_summary_length), dtype = torch.long)\n",
        "        self.attention_mask = torch.ones(self.total_lines,dtype = torch.long)\n",
        "\n",
        "        # Processes the given file, by creating the summaries and texts tensors (each line of these tensors is a list of word indices)\n",
        "        for index, row in tqdm(df.iterrows()):\n",
        "            if index == 0: # We are not interested in the first line (contains the names of the columns)\n",
        "                pass\n",
        "            text, summary = row['Text'], row['Summary']\n",
        "            # The texts which are longer than max_text_length are truncated, the others are padded (thanks to the inialization of the tensors)\n",
        "            try :\n",
        "                text_tokenizer = tokenizer(text, return_tensors=\"pt\", max_length=self.max_text_length, truncation=True, padding='max_length')\n",
        "                summary_tokenizer = tokenizer(summary, return_tensors=\"pt\", max_length=self.max_summary_length, truncation=True, padding='max_length')\n",
        "            except :\n",
        "                print(text)\n",
        "                print(summary)\n",
        "            self.input_ids[index-1, :self.max_text_length] = text_tokenizer['input_ids']\n",
        "            self.attention_mask[index-1] = torch.argmin(text_tokenizer['input_ids'])\n",
        "            self.label_ids[index-1] = summary_tokenizer['input_ids']\n",
        "        \n",
        "        print(f'Processed {self.total_lines} lines.')\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.total_lines\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        attention_item = torch.zeros((1,self.max_text_length), dtype = torch.long)\n",
        "        attention_item[ : self.attention_mask[index]] = 1\n",
        "        return self.input_ids[index], attention_item, self.label_ids[index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set of the maximum length of both the texts and the summaries\n",
        "\n",
        "max_text_length=500\n",
        "max_summary_length=40\n",
        "max_length =  max_summary_length + max_text_length"
      ],
      "metadata": {
        "id": "2TWgvkIX8GEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the dataset\n",
        "\n",
        "dataset = SummarizationDataset('Reviews_preprocessed.csv', max_text_length=max_text_length, max_summary_length=max_summary_length, stop=50000)"
      ],
      "metadata": {
        "id": "WJnqYKjnSr96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train/validation/test datasets\n",
        "\n",
        "total_size = len(dataset)\n",
        "training_size = int(0.8 * total_size)\n",
        "validation_size = int(0.1 * total_size)\n",
        "test_size = total_size - training_size - validation_size\n",
        "\n",
        "training_set, validation_set, test_set = random_split(dataset, [training_size, validation_size, test_size])"
      ],
      "metadata": {
        "id": "fK5730lbS_X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets in DataLoaders\n",
        "\n",
        "batch_size = 4\n",
        "train_dataset_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)  \n",
        "valid_dataset_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True) \n",
        "test_dataset_loader  = DataLoader(test_set , batch_size=1, shuffle=True)\n",
        "print(\"train size : \" , len(training_set))\n",
        "print(\"validation size : \" , len(validation_set))\n",
        "print(\"test size : \" , len(test_set))"
      ],
      "metadata": {
        "id": "JTHZ5s0MEEiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and prepare for training\n",
        "\n",
        "lr = 1e-4\n",
        "weight_decay = 0.01\n",
        "epochs = 4\n",
        "\n",
        "# Load the model: either GPT-2 if no model is stored, or 'model.pt' if you have already started to fine-tune and saved the latest version\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        model.load_state_dict(torch.load(\"model.pt\",map_location=torch.device(\"cuda\")))\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(\"model.pt\",map_location=torch.device('cpu')))\n",
        "except:\n",
        "    print(\"No model find\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=50256)\n",
        "word_embedding = model.wte.weight\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA\")\n",
        "    model.cuda()\n",
        "    word_embedding.cuda()"
      ],
      "metadata": {
        "id": "bWR4ZnZEYy9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to compute and print the ROUGE scores\n",
        "\n",
        "def rouge_scores(dataset_loader) :\n",
        "    predictions = []\n",
        "    summaries = []\n",
        "\n",
        "    metric = load_metric(\"rouge\")\n",
        "    for data in tqdm(dataset_loader):\n",
        "        text, mask, summary = data\n",
        "        if torch.cuda.is_available():\n",
        "            text,mask ,summary= text.cuda(), mask.cuda(), summary.cuda()\n",
        "        last_hidden_states = model(input_ids=text, attention_mask=mask, output_hidden_states=False).last_hidden_state[0,:max_summary_length]\n",
        "        scores = last_hidden_states @ word_embedding.T # compute the logits for every output\n",
        "        prediction = torch.argmax(scores,dim=-1) # take the largest logits as prediction\n",
        "\n",
        "        summary_str = tokenizer.decode(summary[0],skip_special_tokens=True)\n",
        "        text_predicted = tokenizer.decode(prediction[0],skip_special_tokens=True)\n",
        "\n",
        "        predictions.append(text_predicted)\n",
        "        summaries.append(summary_str)\n",
        "        \n",
        "    result = metric.compute(predictions=predictions, references=summaries, use_stemmer=True)\n",
        "    rouge1_result = result[\"rouge1\"].mid.fmeasure * 100\n",
        "    rouge2_result = result[\"rouge2\"].mid.fmeasure * 100\n",
        "    rougeL_result = result[\"rougeL\"].mid.fmeasure * 100\n",
        "\n",
        "    print(\"Rouge 1 : \",rouge1_result)\n",
        "    print(\"Rouge 2 : \",rouge2_result)\n",
        "    print(\"Rouge L : \",rougeL_result)\n"
      ],
      "metadata": {
        "id": "gWLgz1DE0Xez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of the model\n",
        "\n",
        "rouge_scores(valid_dataset_loader)\n",
        "torch.save(model.state_dict(),\"model.pt\")\n",
        "\n",
        "for i in range(epochs):\n",
        "    cpt = 0\n",
        "    for batch in tqdm(train_dataset_loader):\n",
        "        cpt += 1\n",
        "        text, mask, summary = batch\n",
        "        if torch.cuda.is_available(): # load the batch on the gpu if available\n",
        "            text,mask ,summary= text.cuda(), mask.cuda(), summary.cuda()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids=text, attention_mask=mask, output_hidden_states=False).last_hidden_state[:,:max_summary_length] # compute the output of the model\n",
        "        scores = output @ word_embedding.T # compute the logits for every output\n",
        "        loss = criterion(scores.view(-1, scores.size(-1)), summary.view(-1)) # compute the loss\n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step()\n",
        "        if cpt % 5000 == 0:\n",
        "            if torch.cuda.is_available():\n",
        "                loss.cpu()\n",
        "            print('Batch {}: {}'.format(cpt, loss.item()))\n",
        "\n",
        "    torch.save(model.state_dict(),\"model.pt\")\n",
        "    rouge_scores(valid_dataset_loader)"
      ],
      "metadata": {
        "id": "Q1o8bhnrcU0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on a few examples\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cpu()\n",
        "    word_embedding.cpu()\n",
        "\n",
        "texts = [\n",
        "    \"summarize: I recently tried this flavor/brand and was surprised at how delicious these chips are.  The best thing was that there were a lot of 'brown' chips in the bsg (my favorite), so I bought some more through amazon and shared with family and friends.  I am a little disappointed that there are not, so far, very many brown chips in these bags, but the flavor is still very good.  I like them better than the yogurt and green onion flavor because they do not seem to be as salty, and the onion flavor is better.  If you haven't eaten Kettle chips before, I recommend that you try a bag before buying bulk.  They are thicker and crunchier than Lays but just as fresh out of the bag.\",\n",
        "    \"summarize: Great chips, great price.  Odds are that you will have them to yourself.  If you like your fish and chips soaked in malt vinegar, you will love these.  The best salt and vinegar chips I have ever had (and I love this flavor, mind you).  They are spectacular with deli sandwiches or on their own.  Since I have found that S&V is palatable to only the most intelligent of our species I know that my afternoon snack is all mine, unless of course I run into another heavy brained, hearty breathed like mind.  -Summary- If you like the taste of bitter sweet salty vinegar and a crispy chip to boot you wont pucker at the sight of these handily bagged morsels.  It is the first and only time I have committed to a whole case of chips and I will do it again.  Mmmmm... vinegar...\",\n",
        "    \"summarize: These chips taste awesome. And unlike most other flavored chips, they actually make sure that plenty of the flavory salty goodness gets on each individual chip. Just don't pass gas near any pretty ladies after consumption. They'll totally know it was you.\",\n",
        "    \"summarize: So I got this and tasted it strait out of the bottle, it tasted like smoky flavored milk - YUCK! I was depressed I was stuck with 4 bottles of this.  It sat on my shelf and I forgot about it. Last weekend I tasted a Zevia cream soda and was not pleased with the flavor, it tasted weird and smokey like the LorAnn oil I had gotten.  I tried to doctor up the soda with a little SF Vanilla Torani syrup and a tablespoon of heavy cream - IT TASTED AMAZING!!!  The odd smokiness was gone and it had a wonderful rich mouthfeel.<br /><br />So I wanted to see if this would help the LorAnn oil too.  I put a cup of whipping cream in a bowl and added a teaspoon of vanilla and tasted it, it tasted fine.  Then I added ONE DROP of the oil, it is very strong, and stirred it in - IT TASTED SO MUCH BETTER!  I whipped it up and put it on fruit and it was such a treat.<br /><br />Now I will always whip a drop of it with vanilla into desserts and whip cream.  It was such a happy accident to find out how to use it.  When added to vanilla it really adds a new depth of flavor and tastes so different than just out of the bottle or on its own in cream.<br /><br />It is VERY potent so only use a drop and increase after tasting, if you add too much it will ruin the recipe so use a light touch.\",\n",
        "]\n",
        "\n",
        "summaries = [\n",
        "    \"Best sour cream & onion chip I've had\", \n",
        "    \"So Delicious...Yet my companions wont touch them.\",\n",
        "    \"So much flavor your farts will smell like sweet onions\",\n",
        "    \"Do not taste from bottle! Mix with vanilla for true flavor.\",\n",
        "]\n",
        "\n",
        "def test(example) :\n",
        "    inputs =  tokenizer(example, return_tensors=\"pt\", max_length=max_length, truncation=True, padding='max_length')\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    last_hidden_states = outputs.last_hidden_state[0,:max_summary_length]\n",
        "    scores = last_hidden_states @ word_embedding.T\n",
        "    prediction = torch.argmax(scores,dim=-1)\n",
        "    token = tokenizer.decode(prediction,skip_special_tokens=True)\n",
        "    return(token)\n",
        "\n",
        "for i in range(len(texts)) :\n",
        "    token = test(texts[i])\n",
        "    print('Predicted summary: {}'.format(token))\n",
        "    print('Original summary: {}'.format(summaries[i]))"
      ],
      "metadata": {
        "id": "vVNtFiPDyJX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE score on the test data\n",
        "\n",
        "rouge_scores(test_dataset_loader)"
      ],
      "metadata": {
        "id": "kNl2yYrhyEI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ce00bd-b3e4-4b5f-e605-b98d3849339a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5001/5001 [03:17<00:00, 25.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge 1 :  6.0435119658275\n",
            "Rouge 2 :  0.0\n",
            "Rouge L :  6.057487858627633\n"
          ]
        }
      ]
    }
  ]
}